# -*- coding: utf-8 -*-
"""Tu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jC2n0ei-cWEfm2CXO2nwzt0qO72H_Fc8
"""

import argparse
import glob
import librosa
import torchaudio
from speechbrain.pretrained import EncoderClassifier
import numpy as np
from scipy.io import wavfile
import numpy as np
from scipy.io.wavfile import write
import torch
import os
from scipy import linalg
from scipy.io import wavfile
# from scipy.linalg import dot
import pandas as pd
import seaborn as sns
from pathlib import Path
import random
import json




#define cospair function
classifier = EncoderClassifier.from_hparams(run_opts={"device":'cuda'}, source="speechbrain/spkrec-ecapa-voxceleb", savedir="pretrained_models/spkrec-ecapa-voxceleb")
def cos_pair(a,b):
  return np.dot(a,b.T)/linalg.norm(a)/linalg.norm(b)

def compare(wav_dir, file_csv):
    print("Load wav from " + str(wav_dir))
    list_folder = str(wav_dir) + "/*/"
    print(list_folder)
    list_folder = glob.glob(list_folder)
    classifier = EncoderClassifier.from_hparams(run_opts={"device":'cuda'}, source="speechbrain/spkrec-ecapa-voxceleb", savedir="pretrained_models/spkrec-ecapa-voxceleb")
    #min_mat save min cosine pair of 1 wav; min_path save path of wav
    min_mat = []
    min_path = []
    for le in range(len(list_folder)):
        x = glob.glob(str(list_folder[le]) +  '*.wav')
        
        for _ in range(len(x)):
            frequency, signal = wavfile.read(x[_])
            slice_length = 1.2 # in seconds
            overlap = 0.2 # in seconds
            slices = np.arange(0, len(signal)/frequency, slice_length-overlap, dtype=np.float32)
            i = 0
            audio = []
            matrix_audio = []
            for start, end in zip(slices[:-1], slices[1:]):
                i = i + 1
                start_audio = start * frequency
                end_audio = (end + overlap)* frequency
                audio_slice = signal[int(start_audio): int(end_audio)]
                audio_slice = audio_slice.reshape(1,-1)
                audio_slice = torch.tensor(audio_slice).to('cuda')
                audio_slice = classifier.encode_batch(audio_slice)
                audio_slice = audio_slice.detach().cpu().squeeze()
                audio.append(audio_slice)
            try:
                matrix_audio = [ [0]*(len(audio)) for i in range(len(audio))]
                for i in range(len(audio)):
                    for j in range(len(audio)):
                        matrix_audio[i][j]=(cos_pair(audio[i], audio[j]))
                        # print(matrix_audio)
                mymin = min([min(r) for r in matrix_audio])
                min_mat.append(mymin)
                min_path.append(x[_])
            except:
                print(x[_]) 
                os.remove(x[_])
        print(le)

    data = []
    for i in range(len(min_mat)):
        data.append([min_mat[i],min_path[i]])
    print(data)


    data = pd.DataFrame([min_mat,min_path]) #Each list would be added as a row
    data = data.transpose() #To Transpose and make each rows as columns
    data.columns=['MinCos','Path'] #Rename the columns
    print("Save csv to " + str(file_csv))
    my_file = Path(file_csv)
    try:
        data.to_csv(file_csv)
    except:
        print("No dir name " + str(file_csv))
    finally:
        torch.cuda.empty_cache()

    

def remove(playlist, episode_name, thresh_hold):
    file_csv = pd.read_csv(f'./05_similarity/{playlist}/{episode_name}.csv')
    for i in range(len(file_csv)):
        if ((file_csv['MinCos'][i])<float(thresh_hold)):
            os.remove(file_csv['Path'][i])


def get_audio_file(audio_path, playlist, episode_name, num_files=20):
    episode_path = audio_path
    speaker_dict = {}
    for audio_file in os.listdir(episode_path):
        if audio_file.endswith('.wav'):
            speaker = audio_file.split('_')[1]
            if speaker not in speaker_dict:
                speaker_dict[speaker] = [audio_file]
            else:
                speaker_dict[speaker].append(audio_file)
    for key, value in speaker_dict.items():
        num_audio = min(num_files, len(value))
        selected_files = random.sample(value, num_audio)
        speaker_dict[key] = selected_files
    return speaker_dict

def get_all_audio_file(audio_path, playlist, episode_name):
    episode_path = audio_path
    speaker_dict = {}
    for audio_file in os.listdir(episode_path):
        if audio_file.endswith('.wav'):
            speaker = audio_file.split('_')[1]
            if speaker not in speaker_dict:
                speaker_dict[speaker] = [audio_file]
            else:
                speaker_dict[speaker].append(audio_file)
    for key, value in speaker_dict.items():
        for audio_file in value:
            if audio_file.endswith('.wav'):
                speaker = audio_file.split('_')[1]
                path = os.path.join(episode_path, audio_file)
    return speaker_dict

def get_audio_embeddings(episode_path, playlist, episode_name, speaker_dict):
    
    speaker_embeddings = {}
    torch.set_printoptions(precision=4, sci_mode=False)
    for speaker, audio_files in speaker_dict.items():
        embeddings = []
        for audio_file in audio_files:
            audio_path = os.path.join(episode_path, audio_file)
            frequency, signal = wavfile.read(audio_path)
            signal = torch.tensor(signal.reshape(1,-1)).to('cuda')
            embedding = classifier.encode_batch(signal)
            embeddings.append(embedding)
        speaker_embeddings[speaker] = torch.mean(torch.stack(embeddings), dim=0).detach().cpu().squeeze()
    return speaker_embeddings

def saiba_momoi():
    # Create metadata speaker from all episode
    audio_path = './04_vad'
    # metadata = pd.DataFrame(columns=['playlist_name', 'episode', 'speaker', 'embeddings'])
    data = {}
    for playlist in os.listdir(audio_path):
        data[playlist] = {}
        playlist_path = os.path.join(audio_path, playlist)
        for episode in os.listdir(playlist_path):
            data[playlist][episode] = {}
            episode_path = os.path.join(playlist_path, episode)
            speaker_dict = get_audio_file(episode_path, playlist, episode)
            speaker_embeddings = get_audio_embeddings(episode_path, playlist, episode, speaker_dict)
            for speaker, embedding in speaker_embeddings.items():
                data[playlist][episode][speaker] = embedding.tolist()
                
    # metadata.to_csv('/home4/tuanlha/EXpressiveTTS/dataRawProcess/metadata.csv', index=False)
    # Save metadata to JSON file
    with open('/home4/tuanlha/EXpressiveTTS/dataRawProcess/metadata.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
            
    #       in each episode
    #           get random 20 audio
    #           get average of 20 audio embeddings


    # compute similarity between all episode
def saibamomoi_2():
    audio_path = './04_vad'
    data = {}
    for playlist in os.listdir(audio_path):
        playlist_path = os.path.join(audio_path, playlist)
        for episode in os.listdir(playlist_path):
            episode_path = os.path.join(playlist_path, episode)
            for audio_file in os.listdir(episode_path):
                if audio_file.endswith('.wav'):
                    audio_path = os.path.join(episode_path, audio_file)
                    frequency, signal = wavfile.read(audio_path)
                    signal = torch.tensor(signal.reshape(1,-1)).to('cuda')
                    embedding = classifier.encode_batch(signal)
                    data[audio_path] = embedding.tolist()

    # Save metadata to JSON file
    with open('/home4/tuanlha/EXpressiveTTS/dataRawProcess/metadata_2.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)